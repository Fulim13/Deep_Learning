{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BackPropagation Example: Single Neuron with a Single Input\n",
    "\n",
    "#### Setup\n",
    "- Imagine a single neuron with one input $ x $, a weight $ w $, and a bias $ b $.\n",
    "- The neuron applies a linear transformation followed by an activation function. Here, we'll use the identity function (i.e., no activation function) for simplicity.\n",
    "- The output of the neuron is $ y = wx + b $.\n",
    "\n",
    "#### Initial Values\n",
    "- Input $ x = 2 $\n",
    "- Weight $ w = 0.5 $\n",
    "- Bias $ b = 1 $\n",
    "- Target output $ y_{\\text{target}} = 4 $\n",
    "\n",
    "#### Forward Pass\n",
    "1. Compute the output $ y $:\n",
    "   $$\n",
    "   y = wx + b = 0.5 \\times 2 + 1 = 2\n",
    "   $$\n",
    "\n",
    "2. Compute the error (loss) using the Mean Squared Error (MSE) loss function:\n",
    "   $$\n",
    "   \\text{Loss} = \\frac{1}{2} (y_{\\text{target}} - y)^2 = \\frac{1}{2} (4 - 2)^2 = 2\n",
    "   $$\n",
    "\n",
    "#### Backward Pass (Backpropagation)\n",
    "1. Compute the gradient of the loss with respect to the output $ y $:\n",
    "   $$\n",
    "   \\frac{\\partial \\text{Loss}}{\\partial y} = y - y_{\\text{target}} = 2 - 4 = -2\n",
    "   $$\n",
    "\n",
    "2. Compute the gradient of the output $ y $ with respect to the weight $ w $ and the bias $ b $:\n",
    "   $$\n",
    "   \\frac{\\partial y}{\\partial w} = x = 2\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial y}{\\partial b} = 1\n",
    "   $$\n",
    "\n",
    "3. Compute the gradient of the loss with respect to the weight $ w $ and the bias $ b $ using the chain rule:\n",
    "   $$\n",
    "   \\frac{\\partial \\text{Loss}}{\\partial w} = \\frac{\\partial \\text{Loss}}{\\partial y} \\times \\frac{\\partial y}{\\partial w} = -2 \\times 2 = -4\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial \\text{Loss}}{\\partial b} = \\frac{\\partial \\text{Loss}}{\\partial y} \\times \\frac{\\partial y}{\\partial b} = -2 \\times 1 = -2\n",
    "   $$\n",
    "\n",
    "#### Update Weights\n",
    "- Use a learning rate $ \\eta $ (let's say $ \\eta = 0.1 $) to update the weight $ w $and the bias $ b $:\n",
    "  $$\n",
    "  w_{\\text{new}} = w - \\eta \\frac{\\partial \\text{Loss}}{\\partial w} = 0.5 - 0.1 \\times (-4) = 0.5 + 0.4 = 0.9\n",
    "  $$\n",
    "  $$\n",
    "  b_{\\text{new}} = b - \\eta \\frac{\\partial \\text{Loss}}{\\partial b} = 1 - 0.1 \\times (-2) = 1 + 0.2 = 1.2\n",
    "  $$\n",
    "\n",
    "#### Repeat\n",
    "- The process of forward pass, computing the loss, backpropagation, and updating weights is repeated for multiple iterations until the loss is minimized.\n",
    "\n",
    "### Summary\n",
    "1. **Forward Pass**: Compute the output and loss.\n",
    "2. **Backward Pass**: Calculate gradients of the loss with respect to the weights and biases.\n",
    "3. **Update Weights**: Adjust the weights and biases using the gradients and a learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extend the example with a single input, a hidden layer with one neuron, and a single output, we can follow a similar process, but now with additional parameters and layers.\n",
    "\n",
    "### Setup\n",
    "- Input: $ x $\n",
    "- Weights: $ w_1 $ for input to hidden layer, $ w_2 $ for hidden layer to output\n",
    "- Biases: $ b_1 $ for hidden layer, $ b_2 $ for output layer\n",
    "- Target output: $ y_{\\text{target}} $\n",
    "\n",
    "### Initial Values\n",
    "- $x = 2 $\n",
    "- $ w_1 = 0.5 $\n",
    "- $ b_1 = 1 $\n",
    "- $ w_2 = 0.5 $\n",
    "- $ b_2 = 1 $\n",
    "- $ y_{\\text{target}} = 4 $\n",
    "\n",
    "### Forward Pass\n",
    "1. Compute the output of the hidden layer (using the identity function):\n",
    "   $$\n",
    "   h = w_1 x + b_1 = 0.5 \\times 2 + 1 = 2\n",
    "   $$\n",
    "\n",
    "2. Compute the final output $ y $ (using the identity function):\n",
    "   $$\n",
    "   y = w_2 h + b_2 = 0.5 \\times 2 + 1 = 2\n",
    "   $$\n",
    "\n",
    "3. Compute the error (loss) using the Mean Squared Error (MSE) loss function:\n",
    "   $$\n",
    "   \\text{Loss} = \\frac{1}{2} (y_{\\text{target}} - y)^2 = \\frac{1}{2} (4 - 2)^2 = 2\n",
    "   $$\n",
    "\n",
    "### Backward Pass (Backpropagation)\n",
    "1. Compute the gradient of the loss with respect to the output $ y $:\n",
    "   $$\n",
    "   \\frac{\\partial \\text{Loss}}{\\partial y} = y - y_{\\text{target}} = 2 - 4 = -2\n",
    "   $$\n",
    "\n",
    "2. Compute the gradient of the output $ y $ with respect to the hidden output $ h $, weight $ w_2 $, and bias $ b_2 $:\n",
    "   $$\n",
    "   \\frac{\\partial y}{\\partial h} = w_2 = 0.5\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial y}{\\partial w_2} = h = 2\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial y}{\\partial b_2} = 1\n",
    "   $$\n",
    "\n",
    "3. Compute the gradient of the hidden output $ h $ with respect to the weight $ w_1 $, input $ x $, and bias $ b_1 $:\n",
    "   $$\n",
    "   \\frac{\\partial h}{\\partial w_1} = x = 2\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial h}{\\partial b_1} = 1\n",
    "   $$\n",
    "\n",
    "4. Compute the gradient of the loss with respect to the weights $ w_2 $ and $ w_1 $, and biases $ b_2 $ and $ b_1 $:\n",
    "   $$\n",
    "   \\frac{\\partial \\text{Loss}}{\\partial w_2} = \\frac{\\partial \\text{Loss}}{\\partial y} \\times \\frac{\\partial y}{\\partial w_2} = -2 \\times 2 = -4\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial \\text{Loss}}{\\partial b_2} = \\frac{\\partial \\text{Loss}}{\\partial y} \\times \\frac{\\partial y}{\\partial b_2} = -2 \\times 1 = -2\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial \\text{Loss}}{\\partial h} = \\frac{\\partial \\text{Loss}}{\\partial y} \\times \\frac{\\partial y}{\\partial h} = -2 \\times 0.5 = -1\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial \\text{Loss}}{\\partial w_1} = \\frac{\\partial \\text{Loss}}{\\partial h} \\times \\frac{\\partial h}{\\partial w_1} = -1 \\times 2 = -2\n",
    "   $$\n",
    "   $$\n",
    "   \\frac{\\partial \\text{Loss}}{\\partial b_1} = \\frac{\\partial \\text{Loss}}{\\partial h} \\times \\frac{\\partial h}{\\partial b_1} = -1 \\times 1 = -1\n",
    "   $$\n",
    "\n",
    "### Update Weights\n",
    "- Use a learning rate $ \\eta $ (let's say $ \\eta = 0.1 $) to update the weights $ w_1 $, $ w_2 $ and biases $ b_1 $, $ b_2 $:\n",
    "  $$\n",
    "  w_{1_{\\text{new}}} = w_1 - \\eta \\frac{\\partial \\text{Loss}}{\\partial w_1} = 0.5 - 0.1 \\times (-2) = 0.5 + 0.2 = 0.7\n",
    "  $$\n",
    "  $$\n",
    "  b_{1_{\\text{new}}} = b_1 - \\eta \\frac{\\partial \\text{Loss}}{\\partial b_1} = 1 - 0.1 \\times (-1) = 1 + 0.1 = 1.1\n",
    "  $$\n",
    "  $$\n",
    "  w_{2_{\\text{new}}} = w_2 - \\eta \\frac{\\partial \\text{Loss}}{\\partial w_2} = 0.5 - 0.1 \\times (-4) = 0.5 + 0.4 = 0.9\n",
    "  $$\n",
    "  $$\n",
    "  b_{2_{\\text{new}}} = b_2 - \\eta \\frac{\\partial \\text{Loss}}{\\partial b_2} = 1 - 0.1 \\times (-2) = 1 + 0.2 = 1.2\n",
    "  $$\n",
    "\n",
    "### Summary\n",
    "1. **Forward Pass**: Compute the output of the hidden layer, then the final output, and calculate the loss.\n",
    "2. **Backward Pass**: Calculate gradients of the loss with respect to weights and biases in both the hidden and output layers.\n",
    "3. **Update Weights**: Adjust weights and biases using the gradients and a learning rate.\n",
    "\n",
    "Repeat this process for multiple iterations until the loss is minimized. This iterative process gradually adjusts the weights and biases to minimize the error between the predicted output and the target output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "[[-0.16595599  0.44064899 -0.99977125 -0.39533485]\n",
      " [-0.70648822 -0.81532281 -0.62747958 -0.30887855]\n",
      " [-0.20646505  0.07763347 -0.16161097  0.370439  ]]\n",
      "[[-0.5910955 ]\n",
      " [ 0.75623487]\n",
      " [-0.94522481]\n",
      " [ 0.34093502]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "def relu(x):\n",
    "    return (x > 0) * x # returns x if x > 0\n",
    "                       # return 0 otherwise\n",
    "\n",
    "def relu2deriv(output):\n",
    "    return output>0 # returns 1 for input > 0\n",
    "                    # return 0 otherwise\n",
    "\n",
    "streetlights = np.array( [[ 1, 0, 1 ],\n",
    "                          [ 0, 1, 1 ],\n",
    "                          [ 0, 0, 1 ],\n",
    "                          [ 1, 1, 1 ] ] )\n",
    "\n",
    "walk_vs_stop = np.array([[ 1, 1, 0, 0]]).T\n",
    "\n",
    "print(walk_vs_stop)\n",
    "    \n",
    "alpha = 0.2\n",
    "hidden_size = 4\n",
    "\n",
    "weights_0_1 = 2*np.random.random((3,hidden_size)) - 1\n",
    "weights_1_2 = 2*np.random.random((hidden_size,1)) - 1\n",
    "\n",
    "print(weights_0_1)\n",
    "print(weights_1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "Error:4.522200282839368e-15\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "Error:1.9687385946013715e-16\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "Error:8.570895916698259e-18\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "Error:3.73133512041459e-19\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "Error:1.6244351039525356e-20\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "ff0 [[1 0 1]]\n",
      "ff1 [[0 1 1]]\n",
      "ff2 [[0 0 1]]\n",
      "ff3 [[1 1 1]]\n",
      "Error:7.072028248288851e-22\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for iteration in range(60):\n",
    "   layer_2_error = 0\n",
    "   for i in range(len(streetlights)):\n",
    "      layer_0 = streetlights[i:i+1]\n",
    "      layer_1 = relu(np.dot(layer_0,weights_0_1))\n",
    "      layer_2 = np.dot(layer_1,weights_1_2)\n",
    "\n",
    "      layer_2_error += np.sum((layer_2 - walk_vs_stop[i:i+1]) ** 2)\n",
    "\n",
    "      layer_2_delta = (layer_2 - walk_vs_stop[i:i+1])\n",
    "      layer_1_delta=layer_2_delta.dot(weights_1_2.T)*relu2deriv(layer_1)\n",
    "\n",
    "      weights_1_2 -= alpha * layer_1.T.dot(layer_2_delta)\n",
    "      weights_0_1 -= alpha * layer_0.T.dot(layer_1_delta)\n",
    "\n",
    "   if(iteration % 10 == 9):\n",
    "      print(\"Error:\" + str(layer_2_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
